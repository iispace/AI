# 제목 

- **[Intelligent mobile robot navigation in unknown and complex environment using reinforcement learning technique](https://www.nature.com/articles/s41598-024-72857-3)**

# 저자

- Ravi Raj, Andrzej Kos

# Abstract

-  강화학습을 활용해 직접 경험을 통해 학습하는 방법으로, 이동 로봇(MRs, Mobile Robots)이 낯선 환경에서도 스스로 길을 찾고 장애물을 회피하면서 목표 지점까지 이동할 수 있는 방법을 제시함.
-  로봇의 움직임과 환경을 수학적 모델로 표현하고, 로봇이 어떤 상황에서 어떤 행동을 해야 하는지를 결정하는 "정책(policy)"을 신경망으로 학습함.
-  시행착오(trial and error)를 통해, 로봇이 환경을 탐험하면서 행동하고, 그 결과에 따라 보상(reward)을 받음
  - 목표 지점에 가까워지면 긍정적 보상
  - 장애물에 부딪히면 부정적 보상
-  사용 알고리즘:
   - Deep Q-Learning(DQN)
     - Q-Learning을 신경망과 결합한 방식
     - 로봇이 "현재 상태에서 어떤 행동을 하면 가장 좋은 결과가 나올까?"를 학습
   - Epsilon-Greedy 전략
     - 학습 중에는 탐험(exploration)과 활용(exploitation)을 균형 있게 수행
     - 대부분은 현재까지 가장 좋은 행동을 선택하지만, 일정 확률($\varepsilon$ )로 새로운 행동을 시도해 더 나은 방법을 찾음
- 로봇은 낯선 환경에서도 스스로 장애물을 회피하고 목표 지점까지 이동할 수 있게 됨
- 시뮬레이션을 통해 기존 방식보다 효율성과 안전성이 향상됨을 확인함.

# Keywords:

# Introduction

- 경직된 데이터와 메모리 제약 조건에서도 사용할 수 있도록 두 가지 기법을 개발함
  - Monte Carlo learning
  - Temporal-Difference (TD) learning
  - Bellman 방정식과 Markov decision process (MDP) 등의 개념이 TD 학습과 통합되어 Q-Learning 접근 방식이 만들어 짐.
  - Q-Learning 방식의 개발 이후 강화학습 기술은 엄청난 발전을 이루었고, 강화학습 개념은 다양한 실제 문제, 특히 이동 로봇의 네비게이션에 구현되었음.
  

# 용어 정리

|용어|설명|
|:-|:-|
|Holonomic robot|모든 제약 조건이 좌표(위치)나 시간의 함수로 표현될 수 있는 로봇<br>- 드론, Omni-wheel robot, mecanum wheel robot 등과 같이 모든 위치(전,후,좌,우,사선)로 즉시 이동하는 운동 능력이 있는 로봇|
|Non-holomonic robot|위치 제약만으로는 설명할 수 없는 운동 제약을 가진 로봇.<br>- 속도와 방향에 의해 움직임이 제한되는 시스템<br>- 대각선으로는 즉시 이동하지 못하는 챠륜형 로봇(바퀴 달린 로봇)과 같이 특정 방향으로는 직접 이동이 불가능한 로봇. 전진/후진과 좌/우 회전을 하나의 "경로"로 조합해야만 대각선 방향으로의 이동이 가능한 로봇|

# Main contributions of this research

- 환경 또는 로봇 역학에 대한 명시적 모델링이 필요치 않음
- 제어 매개변수를 수동으로 조정할 필요 없이 변화하는 환경과 작업에 적응할 수 있음.
- 경험을 통해 배우고 시간이 지남에 따라 성능을 개선할 수 있음
- 알려지지 않은 환경(unknown environment)에서 이동 로봇을 성공적으로 제어한 연구는 많지 않은데, 본 논문은 종단 간 이동 제어 기술을 제시하여 이전에 논의된 기술의 제약을 극복하고 알려지지 않고 복잡한 환경에서 이동 로봇이 효과적으로 작동할 수 있도록 지원함.

# Limitations

- 제안된 접근법은 시뮬레이션 환경에서 수행되므로 실제 적용 시나리오에 따라 변동이 발생할 수 있음.
- 성능 향상을 위해 향후 보상 함수의 수정 연구가 필요함.

# Literature survey

- (Zeng et al.) 동적 장애물이 많은 환경에서 안정적인 제어를 유지하며 기동할 수 있 DRL 기법 제안 (2019): 연속적인 가속 명령을 지속적으로 산출해 실시간 회피와 부드러운 궤적 전환을 유도함으로써, 기존 기법 대비 어려운 환경에서 우수한 기동성능 달성
- (Li et al.) Deep reinforcement learning-based automatic exploration for navigation (2020)
- (Singla et al.) Memory-based deep reinforcement learning (2021)
- (Wang et al.) 계층형 내비게이션 시스템(2022): 모델(LSTM) 기반 제어와 Deep Reinforcement Learning(DRL) 기반 지각(Soft Actor-Critic: SAC algorithms)을 통합한 계층형 내비게이션 시스템 제안. DRL 모듈의 역할은 "어디로 가야 안전하고 효율적인가"를 학습적으로 판단하고, 저수준의 모델 기반 제어는 "어떻게 정확히 움질일 것인가"를 담당.
- (Cai et al.) Linear Temporal Logic (LTL) formula-based deep policy gradient technique: "A를 지나 B로 가되, C에 절대 접근하지 말고, D를 반복적으로 관찰하라" 같은 순서·빈도·회피 조건을 수학적으로 정확히 표현할 수 있어, 단순한 스칼라 보상으로는 표현하기 어려운 임무 논리를 안전하게 수행할 수 있음
- (Yan et al.) option-based hierarchy DDRL을 사용하는 분산 내비게이션 아키텍처(2023) : 로봇별로 저수준과 고수준의 두 제어 계층을 두고, 각 계층이 학습된 정책으로 행동을 생성,선택하여 전역 통신 없이도 충돌을 피하며 목표로 이동하도록 설계
- (Ali et al.) Use DRL with external and curiosity-driven reward mechanisms 
- (Xue et al.) perception-constrained navigation is represented as a partially observable MDP and an actor-critic-based rapid recurrent stochastic valued gradient method has been developed for a real-time solution: 센서 범위, 거리 제한, 가림(occlusion), 잡음 등으로 환경을 완전하게 볼 수 없으므로, 상태가 전부 관측되지 않은 partially observable MDP(Markov Decision Process)로 표현. 이 때 정책은 현재 관측뿐 아니라 과거 관측의 축적(메모리)을 통해 숨은 상태를 추정
  - 상태: UAV자세,속도와 센서로 추정한 장애물 근접도, 목표 방향 등
  - 관측: 카메라/라이다 등 제한된 감지 결과(잡음 포함)
  - 보상: 충돌 회피, 목표 접근, 비행 효율(시간/에너지)균형을 반영해 밀집 장애물 환경에서 안전하고 효율적으로 학습
- (Zhu et al.) 샘플링 효율과 시뮬레이션-현실 전이 성능이 뛰어난 계층형 DRL 프레임워크 제안(2023): 로봇은 최소 DRL 정책으로 목표를 향해 이동하면서 장애물과의 적절한 거리를 유지하고, 상위 고수준 DRL 정책을 추가해 전반적인 내비게이션 신뢰성을 높임.
  - 로봇을 목적지로 이끄는 경루 위의 waypoint를 sub goal로 선택하여 상태 공간을 축소하고 회소 보상 문제를 완화함. 이렇게 하면 학습 난이도가 줄어들고 수렴 속도가 높아져 샘플링 정확도가 개선됨.
  - sub goal로부터 목표지향적인 행동 공간(action space)을 설계해 이동 정확도를 높이고 불필요한 행동 차원을 줄임.
- (Xue et al.) multi-agent reccurrent deterministic policy gradient(MARDPG) (2024): every critic network(가치 평가 네트워크?)를 중앙 집중식으로 학습시키고, actor(정책 실행)는 분산 방식으로 운용하여 불확실한 환경에서 학습 지연 문제를 완화하고 통신 자원 요구를 줄임.

<br>

# Background information

- 센서 데이터를 수집하고 동시에 주변 환경의 지도를 실제 세계와 좀 더 유사하게 작성하기 위해서는 적절한 탐색 알고리즘이 필요함.
- 주변과 agent 간의 관계는 MDP로 표현 (surroundings, agent actions, and rewards)
- MDP(Markov Decision Process)는 에이전트가 보상 기반 기준을 최대화하기 위해 일련의 행동을 선택해야 하는 반복적 의사 결정 문제를 설명하는데 사용: **M = (S,A,T,R)**
  
  <img width="332" height="400" alt="image" src="https://github.com/user-attachments/assets/e5aa176a-c7b2-4de9-8ab2-32fbb00f15e1" />

- 센서에서 획득한 데이터는 로봇의 행동으로 직접 전환
- 탐색 전략은 경로 계획(planning), 지도화(mapping), 행동(behaviors)의 3가지로 분할하여 처리


<hr>

# Phrases that I may want to recall sometime later

  - By integrating AI and robotics, robots might become intelligent and autonomous, performing tasks without assistance from humans.
  - For a navigational system, one of the most important factors is determining how long and how much energy a robotic device needs to travel a given course.
  - The capability of autonomous learning is not present in the conventional MR(mobile robots) navigation technique. MRs must be able to navigate unknown areas autonomously while avoiding both static and moving obstacles.
  - Nowadays, researchers choose heuristic methods more and more because of the extent to which they imitate the way humans acquire behavior.
  - Among the most well-know heuristic methods is reinforcement learning (RL), which is often used in MR navigation.
  - Some of the key issues with deep learning in robots are that it does not ensure the reliability or accuracy of a learned policy, which presents an additional barrier to learning-based collision prevention. (같은 상황에서도 일관되게 안전한 행동을 선택한다고 보장할 수 없음-신뢰성-. 입력 잡음, 조명, 기후 변화, 센서 오류, 훈련 데이터에 포함되지 않은 분포 상황 등에서 정책의 출력이 흔들릴 수 있음. 또한 충돌 없는 주행이나 올바른 경로 선택을 얼마나 정확히 달성하는지에 대한 체계적 보증도 없음-정확성-. 시물레이션이 잘 되어도 실제 환경에서 동일한 성능을 내지 못할 수도 있다는 한계를 지적하는 표현)
  - The phenomenal modeling capabilities of deep neural networks and the robust interaction, exploration, and self-learning features of RL have led to the widespread use of deel learning, particularly DRL-based techniques, enabling map-free navigation in recent times. (딥 신경망의 놀라운 모델링 능력과 강화학습의 강력한 상호작용, 탐색, 자체 학습 기능으로 인해 딥러닝, 특히 DRL 기반 기술이 널리 사용되어 최근에는 지도 없는 탐색이 가능해졌음)
  - 
