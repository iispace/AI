# 제목 

- [Deep Reinforcement Learning for Sim-to-Real Robot Navigation with a Minimal Sensor Suite for Beach-Cleaning Applications](https://www.mdpi.com/2076-3417/15/19/10719)

# 저자

- Guillermo Cid Ampuero, Gabriel Hermosilla, German Varas, Matias Toribio Clark

# Abstract

- Sim-to-Real transfer of deep reinforcement learning (DRL) policies 연구
- sim-to-real transfer, deep reinforcement learning, minimal sensor suite, PPO-mask, low-cost mobile robotics, autonomous navigation
- 사용 장치: wheel, encoder odometry, a single 2D LiDAR, Raspberry Pi 4
- 시뮬레이션 환경에서 강화학습 훈련에 사용된 policies: Proximal Policy Optimization(PPO) and a masked-action variant(PPO-Mask)
- Gazebo + Gymnasium 시뮬레이션 환경에서 학습 후 물리적 로봇 장치에 하이퍼파라미터 조정 없이 그대로 이식.
- 단단한 모래(firm sand)와 자연 해변의 부드러운 모래(loose-sand beach)에서 로봇이 여러 개의 waypoints(지나가야 하는 좌표들)을 따라가도록 만들고, PPO와 PPO-Mask 중 어느 쪽이 더 잘 움직이는 지 비교
  - PPO: 기본 강화학습 정책
  - PPO-Mask: PPO를 개선해서, 불필요하거나 위험한 행동을 마스킹(Masking)처리 해서 행동하게 만든 정책

- 성능 측정에 사용된 두 가지 지표
  - ISE(Integral of Squared Error):
    - 오차 제급을 실험 시간 동안 모두 더한 값
    - 큰 오차에 더 큰 패널티 부여
    - "전체적으로 얼마나 심하게 벗어났는가"를 강하게 반영
    - 값이 작을수록 성능은 우수
  - IAE(Integral of Absolute Error):
    - 오차의 절댓값을 실험 시간 동안 모두 더한 값
    - 전체적으로 얼마나 벗어나 있었느지를 고르게 평가
    - 값이 작을수록 성능 우수

- 두 개의 정책 효과를 비교한 결과 PPO-Mask가 PPO에 비해 더 우수했음
  - Firm sand 환경에서 PPO에 비해 PPO-Mask 성능이 우수했음: 
    - ISE 기준 16.6% 오차 감소  => 큰 오차를 내는 순간이 줄어들었음을 의미
    - IAE 기준 5.2% 오차 감소
  - 경로 수행 속도 비교 결과:
    - firm sand:
      - Square path under PPO: 112.48 초
      - Square path under PPO-Mask: 103.46초

# Keywords:

|용어|설명|
|:-|:-|
|square path|사각 경로: 4개의 꼭지점을 지나는 경로|
|multi-waypoint paths|A=>B=>C=>D와 같이 여러 지점을 순서대로 지나는 경로|

# 용어 정리

|용어|설명|
|:-|:-|
