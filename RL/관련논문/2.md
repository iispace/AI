# 제목 

- [Deep Reinforcement Learning for Sim-to-Real Robot Navigation with a Minimal Sensor Suite for Beach-Cleaning Applications](https://www.mdpi.com/2076-3417/15/19/10719)

# 저자

- Guillermo Cid Ampuero, Gabriel Hermosilla, German Varas, Matias Toribio Clark

# Abstract

- Sim-to-Real transfer of deep reinforcement learning (DRL) policies 연구
- sim-to-real transfer, deep reinforcement learning, minimal sensor suite, PPO-mask, low-cost mobile robotics, autonomous navigation
- 사용 장치: wheel, encoder odometry, a single 2D LiDAR, Raspberry Pi 4
- 시뮬레이션 환경에서 강화학습 훈련에 사용된 policies: Proximal Policy Optimization(PPO) and a masked-action variant(PPO-Mask)
- Gazebo + Gymnasium 시뮬레이션 환경에서 학습 후 물리적 로봇 장치에 하이퍼파라미터 조정 없이 그대로 이식.
- 단단한 모래(firm sand)와 자연 해변의 부드러운 모래(loose-sand beach)에서 로봇이 여러 개의 waypoints(지나가야 하는 좌표들)을 따라가도록 만들고, PPO와 PPO-Mask 중 어느 쪽이 더 잘 움직이는 지 비교
  - PPO: 기본 강화학습 정책
  - PPO-Mask: PPO를 개선해서, 불필요하거나 위험한 행동을 마스킹(Masking)처리 해서 행동하게 만든 정책

- 성능 측정에 사용된 두 가지 지표
  - ISE(Integral of Squared Error):
    - 오차 제급을 실험 시간 동안 모두 더한 값
    - 큰 오차에 더 큰 패널티 부여
    - "전체적으로 얼마나 심하게 벗어났는가"를 강하게 반영
    - 값이 작을수록 성능은 우수
  - IAE(Integral of Absolute Error):
    - 오차의 절댓값을 실험 시간 동안 모두 더한 값
    - 전체적으로 얼마나 벗어나 있었느지를 고르게 평가
    - 값이 작을수록 성능 우수

- 두 개의 정책 효과를 비교한 결과 PPO-Mask가 PPO에 비해 더 우수했음
  - Firm sand 환경에서 PPO에 비해 PPO-Mask 성능이 우수했음: 
    - ISE 기준 16.6% 오차 감소  => 큰 오차를 내는 순간이 줄어들었음을 의미
    - IAE 기준 5.2% 오차 감소
  - 경로 수행 속도 비교 결과:
    - firm sand:
      - Square path under PPO: 112.48 초
      - Square path under PPO-Mask: 103.46초

# Keywords:

sim-to-real transfer, deep reinforcement learning, minimal sensor suite, PPO-mask, low-cost mobile robotics, autonomous navigation



# 용어 정리

|용어|설명|
|:-|:-|
|square path|사각 경로: 4개의 꼭지점을 지나는 경로|
|multi-waypoint paths|A=>B=>C=>D와 같이 여러 지점을 순서대로 지나는 경로|


# Robot Hardware

<img width="279" height="300" alt="image" src="https://github.com/user-attachments/assets/bef5aab9-98a3-44c5-9d53-6cea68027509" />

# Differential Kinematic Model and Navigation Geometry

- 차동구동 로봇(왼쪽/오른쪽 두 바퀴로 움직이는 로봇)의 위치와 방향이 시간에 따라 어떻게 변하는지를 선속도(linear velocity)와 각속도(angular velocity)로 표현한 수학적 모델
- <img width="495" height="300" alt="image" src="https://github.com/user-attachments/assets/43e8e459-beac-4ef6-8bf0-211ef7ae8d8a" />
- '이 𝑣,𝜔를 주면 𝑥,𝑦,𝜃가 이렇게 변한다'를 말해주는 식
- <img width="325" height="100" alt="image" src="https://github.com/user-attachments/assets/b37f1f04-5cf5-4dea-8d14-98d53e362b2f" />


# Deep Reinforcement Learning

- 강화학습 에이전트는 Markov decision process로 모형화한 환경과 상호작용
  - t: each discrete time step
  - s: state
  - a: action, (a_t : t 시간에서의 action)
    
- 기대 보상 함수(가치 함수)
  - <img width="551" height="72" alt="image" src="https://github.com/user-attachments/assets/d7ac3024-b97b-4d26-b154-15a2b566071c" />
  - <img width="501" height="140" alt="image" src="https://github.com/user-attachments/assets/81b12e82-bec4-4dbb-9fe2-269e85b5f052" />
  
  - γ가 1에 가까울 수록 장기적 관점 지향(agent가 far-sighted한 경우)
    - <img width="276" height="135" alt="image" src="https://github.com/user-attachments/assets/96b4cb4b-dc75-48c1-8a19-f456ac3d3ba9" />
    - 100 step 뒤와 같이 아주 멀리 가도 이 할인 인자(discount factor)의 값이 완전히 0이 되지는 않으므로, 장기적인 보상에도 어느 정도 중요성을 부여할 수 있게 함.
      
  - γ가 0에 가까울 수록 단기적 관점 지향(agent가 myopic한 경우)
    - <img width="249" height="139" alt="image" src="https://github.com/user-attachments/assets/a95a16d8-d177-4902-bcf0-ca90fd005e31" />
    - 10 step 뒤만 되어도 이 할인 인자(discount factor)의 값이 거의 0에 가까워지므로, 단기 보상에 집중하게 함.

  - 가치 함수의 값이 무한대로 발산하지 않게 하기 위해 0<γ<1 의 범위 설정
    
# 결과

<img width="626" height="313" alt="image" src="https://github.com/user-attachments/assets/7f9c59a7-8036-43a1-b6d3-b5ef1a152b56" />
